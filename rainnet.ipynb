{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9876971,"sourceType":"datasetVersion","datasetId":6063872}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy\nfrom sklearn.model_selection import train_test_split\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt \nimport imageio\nimport PIL\nfrom PIL import ImageFile\nimport cv2\nimport numpy as np\nfrom IPython.display import display\nfrom tqdm import tqdm\nfrom imblearn.over_sampling import RandomOverSampler\n\nimport math\nimport albumentations\nimport random\nfrom tqdm import tqdm\nimport tensorflow as tf\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n%matplotlib inline\n     ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:00:55.595595Z","iopub.execute_input":"2024-11-18T06:00:55.596158Z","iopub.status.idle":"2024-11-18T06:01:00.626900Z","shell.execute_reply.started":"2024-11-18T06:00:55.596090Z","shell.execute_reply":"2024-11-18T06:01:00.625962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def Scaler(array):\n    return np.log(array + 0.01)\n\ndef invScaler(array):\n    return np.exp(array - 0.01)\n\ndef pad_to_shape(array, from_shape=900, to_shape=928, how='mirror'):\n    padding = int( (to_shape - from_shape)/2 )\n    if how == 'zero':\n        array_padded = np.pad(array, ((0,0), (padding,padding), (padding, padding), (0,0)), mode = 'constant', constant_values = 0)\n    elif how == 'mirror':\n        array_padded = np.pad(array, ((0,0), (padding,padding), (padding,padding), (0,0)), mode = 'reflect')\n    return array_padded\n\ndef pred_to_rad(pred, from_shape=928, to_shape=900):\n    padding = int((from_shape - to_shape)/2)\n    return pred[::, padding:padding+to_shape, padding:padding+to_shape].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:01:00.628500Z","iopub.execute_input":"2024-11-18T06:01:00.629055Z","iopub.status.idle":"2024-11-18T06:01:00.637656Z","shell.execute_reply.started":"2024-11-18T06:01:00.629016Z","shell.execute_reply":"2024-11-18T06:01:00.636635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def data_preprocessing(X):\n    X = np.moveaxis(X, 0, -1)\n    X = X[np.newaxis, ::, ::, ::]\n    X = Scaler(X)\n    X = pad_to_shape(X)\n    return X\n\ndef data_postprocessing(nwcst):\n    nwcst = pred_to_rad(nwcst)\n    nwcst = invScaler(nwcst)\n    nwcst = np.where(nwcst>0, nwcst, 0)\n    return nwcst","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:01:00.638782Z","iopub.execute_input":"2024-11-18T06:01:00.639097Z","iopub.status.idle":"2024-11-18T06:01:00.648410Z","shell.execute_reply.started":"2024-11-18T06:01:00.639065Z","shell.execute_reply":"2024-11-18T06:01:00.647623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Dataset(tf.keras.utils.Sequence):\n    def __init__(\n        self,\n        dataset_dict,\n        image_names,\n        batch_size\n    ):\n        self.keys = image_names\n        self.dataset = dataset_dict\n        self.bs = batch_size\n\n    def get_index(self, i):\n        x = []\n        for j in range(4):\n            try:\n                arr = np.array(self.dataset.get(self.keys[i+j]))\n            except:\n                print(i, j)\n            x.append(arr)\n\n        x = data_preprocessing(x)\n        x = np.squeeze(x)\n        y = np.array(self.dataset[self.keys[i+4]])[np.newaxis, ::, ::]\n        y = data_preprocessing(y)\n        y = np.squeeze(y)\n\n        return x.astype('float32'), y.astype('float32')\n\n\n    def __getitem__(self, index):\n        X = []\n        Y = []\n        for i in range (self.bs*index, self.bs*(index+1)):\n            x, y = self.get_index(i)\n            X.append(x[np.newaxis, :])\n            Y.append(y[np.newaxis, :])\n        return X, Y\n\n    def __len__(self):\n        return (len(self.keys)-4)//self.bs\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:01:00.650388Z","iopub.execute_input":"2024-11-18T06:01:00.650714Z","iopub.status.idle":"2024-11-18T06:01:00.670861Z","shell.execute_reply.started":"2024-11-18T06:01:00.650667Z","shell.execute_reply":"2024-11-18T06:01:00.669963Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import h5py\ndataset_dict = h5py.File('/kaggle/input/rydl-rainnet/RYDL.hdf5', 'r')\nprint(dataset_dict)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:01:00.672838Z","iopub.execute_input":"2024-11-18T06:01:00.673506Z","iopub.status.idle":"2024-11-18T06:01:00.695697Z","shell.execute_reply.started":"2024-11-18T06:01:00.673459Z","shell.execute_reply":"2024-11-18T06:01:00.694595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_names_full = list(dataset_dict.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T06:01:00.696905Z","iopub.execute_input":"2024-11-18T06:01:00.697247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_names = [name for name in tqdm(image_names_full) if name[0:4]>\"2012\"]\ntrain_images = image_names[:int(total_images*0.8)]\nval_images = image_names[int(total_images*0.8):int(total_images*0.9)]\ntest_images = image_names[int(total_images*0.9):]\nprint(len(train_images), len(val_images), len(test_images))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"arr = np.array(dataset_dict.get(image_names[0]))\nprint(arr.shape)\nplt.figure(figsize=(5,5))\nplt.imshow(arr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = Dataset(\n    dataset_dict = dataset_dict,\n    image_names = train_images,\n    batch_size = 1\n)\n\nval_dataset = Dataset(\n    dataset_dict = dataset_dict,\n    image_names = val_images,\n    batch_size = 1\n)\n\ntest_dataset = Dataset(\n    dataset_dict = dataset_dict,\n    image_names = test_images,\n    batch_size = 1\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = []\nY_train = []\nfor i in range(train_dataset.__len__()):\n    X, Y = train_dataset.__getitem__(i)\n    X = np.squeeze(np.array(X))\n    Y = np.squeeze(np.array(Y))\n    X_train.append(X)\n    Y_train.append(Y)\n\ndel train_dataset\ngc.collect()\n\nX_val = []\nY_val = []\nfor i in range(val_dataset.__len__()):\n    X, Y = val_dataset.__getitem__(i)\n    X = np.squeeze(np.array(X))\n    Y = np.squeeze(np.array(Y))\n    X_val.append(X)\n    Y_val.append(Y)\n\ndel val_dataset\ngc.collect()\n\n\nX_test = []\nY_test = []\nfor i in range(test_dataset.__len__()):\n    X, Y = test_dataset.__getitem__(i)\n    X = np.squeeze(np.array(X))\n    Y = np.squeeze(np.array(Y))\n    X_test.append(X)\n    Y_test.append(Y)\n\ndel test_dataset\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(type(X_train), X_train[0].shape)\nprint(type(X_val), X_val[0].shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_dataset(X_data, Y_data):\n    # Tạo tf.data.Dataset từ các mảng numpy\n    dataset = tf.data.Dataset.from_tensor_slices((X_data, Y_data))\n    \n    dataset = dataset.shuffle(buffer_size = 10000)  \n    dataset = dataset.batch(1)  # Định nghĩa kích thước batch\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)  # Tối ưu hóa việc tải dữ liệu\n    \n    return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_set = create_dataset(X_train, Y_train)\ndel X_train, Y_train\ngc.collect()\n\nval_set = create_dataset(X_val, Y_val)\ndel X_val, Y_val\ngc.collect()\n\ntest_set = create_dataset(X_test, Y_test)\n# del X_test, Y_test\n# gc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\n\ndef rainnet(input_shape=(928, 928, 4), mode=\"regression\"):\n\n    inputs = Input(input_shape)\n\n    conv1f = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(inputs)\n    conv1f = Activation(\"relu\")(conv1f)\n    conv1s = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(conv1f)\n    conv1s = Activation(\"relu\")(conv1s)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1s)\n\n    conv2f = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(pool1)\n    conv2f = Activation(\"relu\")(conv2f)\n    conv2s = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(conv2f)\n    conv2s = Activation(\"relu\")(conv2s)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2s)\n\n    conv3f = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(pool2)\n    conv3f = Activation(\"relu\")(conv3f)\n    conv3s = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(conv3f)\n    conv3s = Activation(\"relu\")(conv3s)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3s)\n\n    conv4f = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(pool3)\n    conv4f = Activation(\"relu\")(conv4f)\n    conv4s = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(conv4f)\n    conv4s = Activation(\"relu\")(conv4s)\n    drop4 = Dropout(0.5)(conv4s)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n\n    conv5f = Conv2D(1024, 3, padding='same', kernel_initializer='he_normal')(pool4)\n    conv5f = Activation(\"relu\")(conv5f)\n    conv5s = Conv2D(1024, 3, padding='same', kernel_initializer='he_normal')(conv5f)\n    conv5s = Activation(\"relu\")(conv5s)\n    drop5 = Dropout(0.5)(conv5s)\n\n    up6 = concatenate([UpSampling2D(size=(2, 2))(drop5), conv4s], axis=3)\n    conv6 = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(up6)\n    conv6 = Activation(\"relu\")(conv6)\n    conv6 = Conv2D(512, 3, padding='same', kernel_initializer='he_normal')(conv6)\n    conv6 = Activation(\"relu\")(conv6)\n\n    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3s], axis=3)\n    conv7 = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(up7)\n    conv7 = Activation(\"relu\")(conv7)\n    conv7 = Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(conv7)\n    conv7 = Activation(\"relu\")(conv7)\n\n    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2s], axis=3)\n    conv8 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(up8)\n    conv8 = Activation(\"relu\")(conv8)\n    conv8 = Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(conv8)\n    conv8 = Activation(\"relu\")(conv8)\n\n    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1s], axis=3)\n    conv9 = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(up9)\n    conv9 = Activation(\"relu\")(conv9)\n    conv9 = Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(conv9)\n    conv9 = Activation(\"relu\")(conv9)\n    conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n    \n    if mode == \"regression\":\n        outputs = Conv2D(1, 1, activation='linear')(conv9)\n    elif mode == \"segmentation\":\n        outputs = Conv2D(1, 1, activation='sigmoid')(conv9)\n\n    model = Model(inputs=inputs, outputs=outputs)\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras import backend as K\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\ndef r2_score(y_true, y_pred):\n    SS_res =  K.sum(K.square(y_true - y_pred)) \n    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n    return (1 - SS_res/(SS_tot + K.epsilon()))\n\nmodel = rainnet()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),loss='log_cosh', metrics=['mse', rmse, r2_score])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_filepath = '/kaggle/working/ckpt/checkpoint.weights.h5'\n\nmy_callbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=4)\n    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, save_weights_only=True, save_best_only=True),\n]\n\n\nhistory = model.fit(train_set, epochs = 40, validation_data = val_set, callbacks=my_callbacks)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Vẽ đồ thị mất mát qua các epochs\nplt.plot(history.history['loss'], label='Training loss')\nif 'val_loss' in history.history:\n    plt.plot(history.history['val_loss'], label='Validation loss')\nplt.title('Loss Progression')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = model.evaluate(test_set)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true = data_postprocessing(np.array(Y_test))\ny_pred = data_postprocessing(np.array(model.predict(test_set)))\n\nimport matplotlib.pyplot as plt\n\nnum_images = 3\n\nplt.figure(figsize=(20,10))\nfor i in range(num_images):\n    # Hiển thị ảnh thực tế\n    ax = plt.subplot(2, num_images, i + 1)\n    plt.imshow(y_true[i].squeeze())  \n    plt.title(\"Actual\")\n    plt.axis(\"off\")\n    \n    # Hiển thị ảnh dự đoán\n    ax = plt.subplot(2, num_images, i + 1 + num_images)\n    plt.imshow(y_pred[i].squeeze())  \n    plt.title(\"Predicted\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}